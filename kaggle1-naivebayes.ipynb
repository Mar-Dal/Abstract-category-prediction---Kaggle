{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : A lot of code is taken from the lab session on naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_list = \"!\\-\\\\\\\"#$%&'(*+,./:;<=>?@[\\]^_`{|}~)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations\n",
    "train['Abstract'] = train['Abstract'].str.replace(\"[\"+punc_list+\"\\\\n\"+ \"]\",' ').str.lower().str.replace('\\\\', ' ')\n",
    "test['Abstract'] = test['Abstract'].str.replace(\"[\"+punc_list+\"\\\\n\"+ \"]\",' ').str.lower().str.replace('\\\\', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unique words\n",
    "unique_words = pd.Series(list(set(\" \".join(train['Abstract']).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove one-letter-long words\n",
    "unique_words = unique_words.loc[unique_words.str.len()>=2]\n",
    "unique_words.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize sparse matrix\n",
    "nrows_train = len(train)\n",
    "ncols = len(unique_words)\n",
    "sparse_train = sparse.lil_matrix((nrows_train, ncols), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     
     ]
    }
   ],
   "source": [
    "#fill up the matrix by using a counter to count the word occurences. Note that this will be turned into 0/1 later (impicitly)\n",
    "for i, row in enumerate(train['Abstract'].str.split().values):\n",
    "    if i%100==0:\n",
    "        print(str(i)+\"/\"+str(nrows_train))\n",
    "    dic = Counter(row)\n",
    "    for j, word in enumerate(unique_words):\n",
    "        if word in dic:\n",
    "            sparse_train[i, j] = dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set sparse matrix\n",
    "nrows_test = len(test)\n",
    "sparse_test = sparse.lil_matrix((nrows_test, ncols), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(test['Abstract'].str.split().values):\n",
    "    if i%500==0:\n",
    "        print(str(i)+\"/\"+str(nrows_test))\n",
    "    dic = Counter(row)\n",
    "    for j, word in enumerate(unique_words):\n",
    "        if word in dic:\n",
    "            sparse_test[i, j] = dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame.sparse.from_spmatrix(sparse_train).values\n",
    "y_train = train[\"Category\"].values\n",
    "X_test = pd.DataFrame.sparse.from_spmatrix(sparse_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliMaxLikelihood:\n",
    "    def __init__(self, n_dims):\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "    # The function gets the probabilities of an occurence or an absence of a word given train data\n",
    "    def train(self, train_data):\n",
    "        self.p0 = (((train_data==0).sum(axis=0)+1)/(len(train_data)+2))\n",
    "        self.p1 = (((train_data>=1).sum(axis=0)+1)/(len(train_data)+2))\n",
    "        \n",
    "    # Returns a vector of size nb. of test ex. containing the log probabilities of each test example under the model.\n",
    "    # exemple test\n",
    "    def loglikelihood(self, test_data):\n",
    "        x = test_data\n",
    "        x = np.clip(x, 0, 1)\n",
    "        notx = 1-x\n",
    "        log_prob = np.sum(x*np.log(self.p1), axis=1)+np.sum(notx*np.log(self.p0), axis=1)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self, maximum_likelihood_models, priors):\n",
    "        self.maximum_likelihood_models = maximum_likelihood_models\n",
    "        self.priors = priors\n",
    "        if len(self.maximum_likelihood_models) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.maximum_likelihood_models)\n",
    "\n",
    "    # Returns a matrix of size number of test ex. times number of classes containing the log\n",
    "    # probabilities of each test example\n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        log_pred = np.zeros((test_data.shape[0], self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            print(str(i)+\"/\"+str(15))\n",
    "            # Here, we will have to use maximum_likelihood_models[i] and priors to fill in\n",
    "            # each column of log_pred \n",
    "            result = self.maximum_likelihood_models[i].loglikelihood(test_data)\n",
    "            log_pred[:, i] = result + np.log(self.priors[i])\n",
    "\n",
    "        return log_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initializing the model\n",
    "dim = X_train.shape[1]\n",
    "models = []\n",
    "total_num = []\n",
    "for i, category in enumerate(train[\"Category\"].unique()):\n",
    "    cat = X_train[train[\"Category\"]==category]\n",
    "    model = BernoulliMaxLikelihood(dim)\n",
    "    model.train(cat)\n",
    "    models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the priors\n",
    "model_ml = models\n",
    "total_num = len(train)\n",
    "priors = train[\"Category\"].value_counts().values/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the classifier\n",
    "classifier = BayesClassifier(model_ml, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a number between 0 and 1 representing the accuracy of the model on the test_inputs\n",
    "#Not actually used, but was used for testing purposes\n",
    "def get_accuracy(data, labels, values):\n",
    "    # Calculate the log-probabilities according to our model\n",
    "    log_prob = classifier.loglikelihood(data)\n",
    "    # Predict labels\n",
    "    print(log_prob)\n",
    "    classes_pred = values[log_prob.argmax(1)]\n",
    "    print(classes_pred)\n",
    "    # Calculate the accuracy by comparing the predicted labels with the actual labels\n",
    "    acc = np.mean(classes_pred == labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets predictions\n",
    "def get_predictions(data, values):\n",
    "    log_prob = classifier.loglikelihood(data)\n",
    "    classes_pred = values[log_prob.argmax(1)]\n",
    "    return classes_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "test_preds = get_predictions(X_test, train[\"Category\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>astro-ph.SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>astro-ph.SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>math.AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>14995</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>14996</td>\n",
       "      <td>physics.optics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>14997</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>14998</td>\n",
       "      <td>gr-qc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>14999</td>\n",
       "      <td>cond-mat.mes-hall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id           Category\n",
       "0          0            stat.ML\n",
       "1          1        astro-ph.SR\n",
       "2          2        astro-ph.SR\n",
       "3          3            math.AP\n",
       "4          4              cs.LG\n",
       "...      ...                ...\n",
       "14995  14995           astro-ph\n",
       "14996  14996     physics.optics\n",
       "14997  14997           astro-ph\n",
       "14998  14998              gr-qc\n",
       "14999  14999  cond-mat.mes-hall\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = test[[\"Id\"]].copy()\n",
    "results[\"Category\"] = test_preds\n",
    "results.to_csv(\"naive_to_submit2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
